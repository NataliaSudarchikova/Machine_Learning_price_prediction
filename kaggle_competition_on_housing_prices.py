# -*- coding: utf-8 -*-
"""Kaggle_competition_on_housing_prices.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pLBCh4rhz8L_6Lwlbc5WjhTIK2BKP_xl
"""

!pip install scikit-learn --upgrade

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
from sklearn.tree import DecisionTreeRegressor
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import make_pipeline
from sklearn.compose import ColumnTransformer

url = 'https://drive.google.com/file/d/1eLQtwd-dxX35qb7jQoJT3Ns0BOow8KKX/view?usp=share_link' # train
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
data = pd.read_csv(path)

# setting that we can see all the columns
pd.set_option('display.max_columns', None)

data.head(5)

data.shape

#X_num = list(X_train.select_dtypes(include='number').columns)
#X_txt = list(X_train.select_dtypes(exclude='number').columns)
#conditions = ('Qual', 'Cond', 'Qu', 'QC')
#X_con = []
#for item in X_txt:
#  if item.endswith(conditions):
#    X_con.append(item)
#    X_txt.remove(item)
#qualities = ['None', 'TA', 'Po', 'Fa', 'Gd', 'Ex']

#X_con

"""**We set the certain features to category data type**"""

cat_ordinal = ['ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC','KitchenQual','FireplaceQu','GarageQual','GarageCond','PoolQC']

for col in cat_ordinal:
    data[col] = data[col].astype('category')

"""**Splitting the data, dropping Id column to reduce the noise**"""

X = data.drop(columns="Id").copy()
y = X.pop('SalePrice')

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8, random_state=8)

data.info()

"""**Let's see how dummy model performs**"""

data.corrwith(data['SalePrice']).sort_values()

sns.relplot(x=X_train['GrLivArea'],y=y_train,height=8)

# GrLivArea * 120

dummy_model_predictions = X_test['GrLivArea'] *120
dummy_model_predictions

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_squared_log_error

dummy_rmse = mean_squared_error(y_true = y_test, y_pred = dummy_model_predictions, squared=False)
dummy_rmsle = mean_squared_log_error(y_true = y_test, y_pred = dummy_model_predictions, squared=False)

dummy_rmse

dummy_rmsle

"""**Now, let's create a better model.** 

First, we created pipelines. 

Here we split the data to numerical and categorical.
We apply simple imputer to replace the missing values with the mean values in numeric data and with 'None' and 'NaN' values in categorical data. We also applyed Encoders for the categorical values.
"""

from sklearn.compose import make_column_transformer
from sklearn.compose import make_column_selector
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import OneHotEncoder

qualities = ['None', 'TA', 'Po', 'Fa', 'Gd', 'Ex']

numeric_pipe = make_pipeline(
    SimpleImputer(strategy='mean')
)
categorical_pipe_ordinal = make_pipeline(
    (SimpleImputer(strategy='constant',fill_value='None')),
    (OrdinalEncoder(categories=[qualities]*10))
)
categorical_pipe_onehot = make_pipeline(
    (SimpleImputer(strategy='constant',fill_value='NaN')),
    (OneHotEncoder(handle_unknown='ignore',sparse_output=False))
)

preprocessor = make_column_transformer(
    (numeric_pipe,make_column_selector(dtype_include='number')),
    (categorical_pipe_ordinal,make_column_selector(dtype_include='category')),
    (categorical_pipe_onehot,make_column_selector(dtype_include='object'))
)

dt_pipeline = make_pipeline(
                            preprocessor, 
                            DecisionTreeRegressor()).set_output(transform='pandas')

dt_pipeline

dt_pipeline.fit(X_train,y_train)

dt_predictions = dt_pipeline.predict(X_test)

dt_rmse = mean_squared_error(y_true = y_test, y_pred = dt_predictions, squared=False)
dt_rmsle = mean_squared_log_error(y_true = y_test, y_pred = dt_predictions, squared=False)

dt_rmse

dt_rmsle



"""**Adding feature selection**

SelectFromModel
"""

from sklearn.feature_selection import SelectFromModel
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler

selectfeatures = SelectFromModel(DecisionTreeRegressor())

dt2_pipeline = make_pipeline(
                            preprocessor,
                            selectfeatures, 
                            DecisionTreeRegressor()).set_output(transform='pandas')

dt2_pipeline

dt2_pipeline.fit(X_train,y_train)

dt2_predictions = dt2_pipeline.predict(X_test)

dt2_rmse = mean_squared_error(y_true = y_test, y_pred = dt2_predictions, squared=False)
dt2_rmsle = mean_squared_log_error(y_true = y_test, y_pred = dt2_predictions, squared=False)

dt2_rmse

dt2_rmsle

"""SelectFromModel show not the best results, let's continue with the SearchCV to find the best model parameters.

And change model from DecisionTree to RandomForest.

**RandomizedSearchCV**

Let's find the best model parameters. 

First with RandomizedSearch for the broader look to save the computational time and that we will tune it with the GridSearch for narrow search.
"""

from sklearn.ensemble import RandomForestRegressor

pipe_RandomForest = make_pipeline(
    (preprocessor),
    (RandomForestRegressor(random_state=123))
)

from sklearn.model_selection import RandomizedSearchCV

param_dist = {
    
    'columntransformer__pipeline-1__simpleimputer__strategy':['mean','median'],
    'randomforestregressor__max_depth':[5,10,15,20,25,30,35,40,45,50,55],
    'randomforestregressor__min_samples_leaf':[2,4,6],
    'randomforestregressor__n_estimators':[5,10,30,40],
}

random_search = RandomizedSearchCV(
    pipe_RandomForest,
    param_distributions=param_dist,
    n_iter=50,
    cv=5,
    random_state=123,
    verbose=1,
)

random_search.fit(X_train,y_train)

random_search.best_params_

from sklearn.model_selection import GridSearchCV

param_grid = {
    'randomforestregressor__max_depth':range(39,40,41),
    'randomforestregressor__min_samples_leaf':[1,2],
    'randomforestregressor__n_estimators':range(40,41,42)
}

grid_search = GridSearchCV(
    pipe_RandomForest,
    param_grid=param_grid,
    cv=5,
    verbose=1,
    scoring='neg_mean_squared_log_error'
)

grid_search.fit(X_train,y_train)

grid_search.best_params_

rf_bestestimator = grid_search.best_estimator_.predict(X_test)

rf_rmse = mean_squared_error(y_true = y_test, y_pred = rf_bestestimator, squared=False)
rf_rmsle = mean_squared_log_error(y_true = y_test, y_pred = rf_bestestimator, squared=False)

rf_rmse

rf_rmsle

grid_search.best_estimator_.fit(X,y)

"""**This looks better**

rf_rmsle = 0.0612

**That how it would look like if we do it manually**
"""

rfc_pipeline = make_pipeline(
                            preprocessor,
                            RandomForestRegressor(
                                  random_state=123,
                                  max_depth=39,
                                  min_samples_leaf=2,
                                  n_estimators=40)).set_output(transform='pandas')

rfc_pipeline

rfc_pipeline.fit(X_train, y_train)

manual = rfc_pipeline.predict(X_test)

manual_rmsle = mean_squared_log_error(y_true = y_test, y_pred = manual, squared=False)

manual_rmsle

"""**Let's see linear regression model**"""

from sklearn.linear_model import LinearRegression
import numpy as np

er_pipeline = make_pipeline(preprocessor, 
                            StandardScaler(),
                            LinearRegression()).set_output(transform='pandas')

er_pipeline.fit(X_train, np.log(y_train))

er_predictions = np.exp(er_pipeline.predict(X_test))

er_rmsle = mean_squared_log_error(y_true = y_test, y_pred = er_predictions, squared=False)

er_rmsle

"""Ups, not good at all.

**Now, let's apply the best model I found till now with the best parameters.**

It is RandomForestRegressor with best parameters I found. And now I will apply this trained model to the new data set.
"""

url = "https://drive.google.com/file/d/1HyODh2TjoavvlhXrQhqPkMKz5vDcOiya/view?usp=share_link"
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
data2 = pd.read_csv(path)
data2

id_column = data2.pop('Id')

predictions = grid_search.best_estimator_.predict(data2)

result = pd.DataFrame({'Id':id_column, 'SalePrice':predictions})

result

from google.colab import files
result.to_csv('result_Natalia_DataScientist_3.csv',index=False)
files.download('result_Natalia_DataScientist_3.csv')